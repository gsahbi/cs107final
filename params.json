{
  "name": "Sentiment Analysis of Product Review",
  "tagline": "CS107 Final Project",
  "body": "\r\n![](http://i.imgur.com/wYyRKNn.png)\r\n\r\n# Introduction\r\n## Motivation :\r\nSentiment Analysis, is receiving a big attention these days, because of its huge spectrum of applications ranging from product review analysis, campaign feedback, competition bench-marking, customer profiles, political trends, etc...\r\n\r\nThere is a huge flow of information going through the internet and social networks. Online discussions are only relevant to people for a couple of days. Nobody actually goes in past to tweets that are older than maybe a week, for instance. This entire humanity archive of discussion could help in many applications if we train machines to understand the sentiment of people towards a specific theme at a specific time. \r\n\r\n## Background\r\n“Sentiment analysis is the computational study of people's opinions, sentiments, emotions, and attitudes.” [Excerpt From: Bing Liu. Sentiment Analysis: Mining Opinions, Sentiments, and Emotions.]. This book is an excellent survey of NLP and SA research.\r\n\r\nGiven the large amount of data available on the Web, it is now possible to investigate high-level Information Retrieval tasks like user's intentions and feelings about facts or objects discussed. [Pang, B., Lee, L., 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval] \r\n\r\n# Sentiment Analyis\r\n\r\n## Challenge :\r\nThere are several things to take into consideration when approaching a Sentiment Analysis task. In general, there are two main approaches: \r\n\r\n- Sentiment lexicons using Natural Language Processing (NLP) techniques. A Sentiment lexicon is a list of words that are associated to polarity values (positive or negative). NLP techniques offer a deep level of analysis since they take into account the context words in the sentence. \r\n\r\n- Machine Learning classification algorithms. Because sentiment classification is a text classification problem, any existing supervised learning method can be directly applied [Bing Liu]. For example,  naive Bayes classification , logistic regression, support vector machines (SVM), etc..\r\n\r\nIn this work we'll work on ML classification and then try to get into the NLP and experience some of the basic techniques used.\r\n\r\n\r\n## Data Wrangling and Preparation\r\n\r\n### Load data\r\n\r\nIn this work we'll use a data-set that we obtained thankfully from Julian McAuley, at the University of San Diego (here)[http://jmcauley.ucsd.edu/data/amazon/] \r\n\r\nWe'll be also inspired by their SIGIR and KDD papers (listed on the above page) as a baseline for our accuracy bench-marking.\r\n\r\nThis data-set contains product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014. \r\nWe have decided to use electronics reviews for this work. Because electronics are not perfect so create a lot of contrasted opinions.\r\n \r\n\r\nThe file format is **NDJSON** (Newline Delimited JSON), So `stream_in` is the appropriate way of to load the data into a data frame.\r\n\r\n\r\n\r\n## Exploratory Data Analysis\r\n### Basic numbers\r\nLet's explore some facts about our data.\r\n\r\nChecking how many users are there. Almost we have 46K users for 50K reviews. So each user has done 1 unique review per product.\r\n\r\n\r\n\r\nChecking how many products are there using the distinct ASIN (Amazon Standard Identification Number) amazon's unique product identifier. \r\n\r\n\r\n\r\nLet's look at at the the number of ratings per product. It's quite skewed with some extreme best sellers (a headphone from Koss) having 3000 reviews. \r\n\r\n\r\n\r\n### Simplification of data\r\n\r\nPrepare the text for processing, convert it to lower case, and keep only relevant columns.\r\nWe then garbage collect the old `dat`\r\n\r\n\r\n\r\n\r\n### Rating system in Amazon\r\n\r\nThe rating system used in Amazon is as follows : \r\n\r\n* emotional positive (5 stars)\r\n* rational positive  (4 stars)\r\n* neutral            (3 stars)\r\n* rational negative  (2 stars)\r\n* emotional negative (1 star )\r\n\r\n**The user's star rating of his own review description as a subjective human interpretation of opinion. So, we consider that as the ground truth.** \r\n\r\nLet's see the distribution of these ratings in our case\r\n\r\n\r\n\r\n> The ratings are very skewed towards positive feedback. Which is an indication that Amazon is not selling junk at least but it's not going to help in our modeling. We have to have equal likelihood of each class of the ratings. \r\n\r\n> In the next sections we'll solve this.\r\n\r\n## Design decisions on reviews\r\n\r\nIn this work we will have a binary classification. Either Positive or Negative.\r\nWe objectively chose to demarcate each rating at the 2.x level divide, such that star ratings above this level would be marked as “1” and star ratings below this level would be marked as “0” \r\n\r\nWe will collapse the 3, 4 and 5 stars ratings into “1” value, and the 1 and 2 stars ratings into Negative “0”\r\n\r\nSo at the end we'll have only 2 opinions : negative/positive\r\nThe simplest way to do this is by joining a mapping matrix\r\n\r\n\r\n\r\nLet's check again the distribution of opinions\r\n\r\n\r\n\r\nEven after this mapping the class proportions need to be corrected manually.\r\nWe calculate the skew (disproportion rate)\r\n\r\n\r\n\r\nwe have 3x more Positive than Negative. Let's remove 2/3 of Positive to adjust the proportions\r\n\r\n\r\n\r\nCheck the numbers, we are good to go :\r\n\r\n\r\n\r\n# Machine Learning Classification\r\n\r\n## Bag of Words\r\n\r\nOne of the simpler things to do with text is to treat each text as a \"bag of words\". We have used the `tm` package in order to construct a Term Document Matrix but the computer couldn't handle such  huge dimensions. So let's go with `tidytext`\r\n\r\nLet's discover the top words for both positive and negative ratings. We use the `wordcloud` package to have a nice display. \r\n\r\n\r\n\r\n\r\n\r\n\r\n## Text tidiying\r\n\r\nDo a series of transformations :\r\n\r\n- lowercase\r\n- remove punctuation\r\n- strip white space\r\n\r\n\r\n\r\nUse `nrc` lexicon as a bag of words that have sentiments but we're not going to look into these sentiments for the time being.\r\n\r\n\r\n\r\n\r\n## Creating features from Bag of Words\r\n\r\nUse `unrest_tokens` and join function in order to convert our reviews into a sparse Matrix.\r\n\r\n\r\n\r\n\r\n## TF-IDF\r\n\r\nTerm Frequency - Inverse Document Frequency is term count within a document weighted against the term's ubiquity within the corpus. This weight is based on the principle that terms occurring in almost every document are therefore less specific to an individual document and should be scaled down. \r\nSo a tf-idf value represents the term's relative importance within a document.\r\n\r\nCompute tf-idf, inverse document frequency, and relative term frequency on document-feature matrices\r\n\r\n$$tf(t,d) = \\frac{f_{d}(t)}{\\underset{w \\in d}{max}}$$\r\n$$idf(t,D) = log \\left (\\frac{|D|}{|d \\in D : t \\in d|}  \\right )$$\r\n$$tfidf(t,d,D) = tf(t,d)\\cdot idf(t,D)$$\r\n$$f_d(t) := freqency\\ of\\ term\\ t\\ in\\ document\\ d$$\r\n$$D : corpus\\ of\\ documents$$\r\n$$|D| : number\\ of\\ documents\\ where\\ the term\\ t\\appears $$\r\n$$|d \\in D : t \\in d|\\ :\\ number\\ of\\ documents\\ where\\ the\\ term\\ t\\ appears$$\r\n\r\n\r\n\r\n\r\n## Reduce dimensionality (Single Vector Decomposition)\r\n\r\nSuppose we have m words (features) and n documents \r\nSingle vector decomposition (SVD) of an m*n real or complex matrix X is a factorization of the form :\r\n\r\n\r\n$$\\mathbf{ X = U \\Sigma V^{T} }$$\r\n$$ U\\ is\\ m x r\\ matrix,\\ columns\\ of\\ U\\ contain\\ the\\ Eigenvectors\\ of\\ X \\cdot X^{T} $$\r\n$$ V\\ is\\ an\\ r x n,\\ columns\\ of\\ V\\ contain\\ the\\ Eigenvectors\\ of\\ X^{T} \\cdot X $$\r\n$$ \\Sigma\\ is\\ a\\ diagonal\\ r x r\\ matrix.\\ diagonal\\ values\\ are\\ eigenvalues\\ of\\ X \\cdot X^{T}$$\r\n\r\n$$\\begin{bmatrix} x_{1,1} & x_{1,2}  & \\cdots & x_{1,n} \\\\ x_{2,1}& \\ddots & & \\vdots \\\\ \\vdots &  & \\ddots & \\vdots \\\\ \r\n x_{m,1}& \\cdots &  \\cdots & x_{m,n} \\\\ \\end{bmatrix}= \\begin{bmatrix} u_{1,1} & u_{1,2}  & \\cdots & u_{1,r} \\\\ u_{2,1}& \\ddots & & \\vdots \\\\ \\vdots &  & \\ddots & \\vdots \\\\u_{m,1}& \\cdots &  \\cdots & u_{m,r} \\\\\\end{bmatrix} \\cdot diag \\begin{bmatrix}d_{1}\\\\\\vdots \\\\  \\vdots  \\\\  d_{r} \\\\ \\end{bmatrix}\\cdot \\begin{bmatrix} v_{1,1} & v_{1,2}  & \\cdots & v_{1,n} \\\\ v_{2,1}& \\ddots & & \\vdots \\\\ \\vdots &  & \\ddots & \\vdots \\\\ v_{r,1}& \\cdots &  \\cdots & v_{r,n} \\\\ \\end{bmatrix}$$\r\n\r\nIt's basically a PCA but without mean shifting.But SVD works better in SA because it is able to detect and extract small signals from noisy data. Noisy data here means words that are not significant for prediction.\r\n\r\nIn this context, it is known as latent semantic analysis (LSA).\r\n\r\n\r\n\r\n\r\nThe diagonal vector `d` generated is ordered by importance of each dimension.\r\nLet's graph a cumulative variance:  \r\n\r\n\r\n\r\n> Design decision\r\n>\r\n>We have to decide how many dimensions to keep. Looking at the graph we can keep 10 dimensions which represent almost 99% of the features.\r\nThis is curious and a puzzling choice to make. But let's aim for fast processing and later on see if adding more dimensions improves our prediction.\r\n\r\nIn order to rotate into our new space of reduced dimensions we have to limit the number of eigenvectors in `U` then transpose it an multiply it by the original X :\r\n\r\n$$ \\hat{X} = U^{T}\\cdot X $$\r\n\r\n\r\n\r\nTransform it back to a dataframe and bring back the truth y\r\n\r\n\r\n\r\n\r\nIf we consider the first 2 dimensions as predictors let's see how it looks\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n## Supervised training\r\n\r\nNow that we know some basic facts about our data set, \r\nlet's randomly split the data into training and test data. \r\nWe set the seed `set.seed(755)` and use `sample()` function to select \r\nyour test index to create two separate data frames\r\ncalled: `train` and `test` from the original `ratings` data frame. \r\n `test` contains a randomly selected 20% of the rows and training the other 80%. We will \r\nuse these data frames to do the rest of the analyses in the problem set.\r\n\r\n\r\n\r\n\r\nAs we go along we will be comparing different approaches. Let's start by creating a benchmark table:\r\n\r\n\r\n\r\n### Naive Bayes supervised training \r\n\r\nThe basic idea to find the probabilities of categories given a text document by using the joint probabilities of words and categories. It is based on the assumption of word independence.\r\nThe starting point is the Bayes theorem for conditional probability, stating that, for a given data point x and class C: \r\n\r\n$$P(C/x) = \\frac{P(x/C) \\cdot P(C)}{P(x)}$$\r\n\r\nBy making the assumption that for a data point x = {x1,x2,…xj}, the probability of each of its attributes occurring in a given class is independent, we can estimate the probability of x as follows :\r\n\r\n$$P(C/x) = P(C) \\cdot \\prod_{j} P(x_{j}/C)$$\r\n\r\n\r\nNow, we can train the naive Bayes model with the training set. We'll be using e1071 package made by David Meyer from TU Wien.\r\nThe naiveBayes function requires a \r\n\r\n\r\n\r\n\r\n### Support Vector Machine \r\n\r\nSVMs were developed by Cortes & Vapnik (1995) [1] for binary classification. Their approach may be roughly sketched as follows:\r\n> Class separation: basically, we are looking for the optimal separating hyperplane between the two classes by maximizing the margin between the classes’ closest points —the points lying on the boundaries are called support vectors, and the middle of the margin is our optimal separating hyperplane;\r\n\r\n[1] : Cortes, C. & Vapnik, V. (1995). Support-vector network. Machine Learning, 20, 1–25\r\n\r\n\r\n\r\n### Logistic regression\r\n\r\n\r\n\r\nIn the previous model we were assuming a less optimal cut-off. Let's use LDA assuming of course that our covariates are bivariate normal\r\n\r\n\r\n\r\n\r\n\r\n### Cross-Validation of Logistic Regression\r\n\r\n\r\n\r\n\r\n### Randon Forest \r\n\r\nThe `randomForest` prediction function works similarly to decision trees\r\n\r\n\r\n \r\n\r\n \r\n\r\n\r\n\r\n## Conclusion\r\n\r\nIn this section we used a supervised machine learning approach and tried several classification models. From the table below, the algorithmic approach using randomForest gives the best accuracy. we are 25% better than flipping a coin with only a classified bag of words.\r\n\r\n\r\n\r\n\r\n\r\n# Sentiment lexicons using Natural Language Processing\r\n\r\n## Feature Extraction and Classification\r\n\r\n\r\nThe first step is to extract 2 types of phrases :\r\n- Verbal phrases  that may imply opinions. Example : \"I didn't like the design\"\r\n- Noun phrases that may describe the product. Example : The design was not good\"\"\r\n\r\nIn our work, we use the OpenNLP chunker. This chunker will split a given text into a sequence of semantically correlated phrases but does not specify their internal structure, nor their role in the main sentence.\r\n\r\nopenNLP `Maxent_Chunk_Annotator` requires a pre-made models. These are conveniently available to R by installing the respective `openNLPmodels.language` package from the repository at http://datacube.wu.ac.at\r\n\r\nTo install English language model (a heavy download 74MB) :\r\n`install.packages(\"openNLPmodels.en\", repos = \"http://datacube.wu.ac.at\")`\r\n\r\n\r\n\r\n\r\nIn the following, we'll experiment with a 1-star rated wireless charger from Amazon. And to evaluate this chuncker we selected a text with a lot of grammar and orthographic mistakes:\r\n\r\nFix punctuation issues in this example \"best.I\" using `gsub` from the `stringr` package.\r\n\r\n\r\n\r\n## Sentence chunking and Part-of-Speech tagging\r\n\r\nThe part-of-speech tags meaning is found in the [Penn Treebank Project](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\r\n\r\nThe chunk tags contain the name of the chunk type, for example I-NP for noun phrase words and I-VP for verb phrase words. Most chunk types have two types of chunk tags, B-CHUNK for the first word of the chunk and I-CHUNK for each other word in the chunk\r\n\r\n\r\n\r\n\r\nTransform it to  [word, pos-tag, chunk-tag] dataframe :\r\n\r\n\r\n\r\nThe first column contains the current word.\r\n\r\nThe second its part-of-speech tag\r\n\r\nThe third its chunk tag.\r\n\r\n### Feature identification\r\n\r\nNow we want to identify features. For the purpose of this work we won't go for a full analysis using penn tree. Rather we'll simply identify the following words inside a verbal phrase:\r\n\r\n- [VB] : verb w/ positive/negative sentiment : like, hate , etc\r\n- [JJ] : adjective w/ positive/negative sentiment : bad, junk\r\n- [RB] : adverb polarity inverter such as : n't, or incr/decrementers such as : too, very, more etc.\r\n\r\nFirst, let's use tidytext sentiments to construct such mapping table\r\n\r\n\r\n\r\n## Scoring algorithm\r\n\r\nNow comes the scoring algorithm part. The idea is to find a VB or JJ and look for a surrounding RB multiplier:\r\n\r\nFor example : n't like : will compute 2 x -1\r\n\r\n\r\n\r\nNow we need to wrap all above into a function \r\n\r\n\r\n\r\nApply it to a number of review texts .\r\n\r\n\r\n\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}