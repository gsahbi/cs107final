---
title: 'CS107 Final Froject : Sentiment Analysis of Product Reviews'
author: "Sahbi Ben Gdaiem"
date: "April 27, 2016"
output: 
  html_document: 
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

TODO : introduce the subject, motivation, approach etc..

## Sentiment Analyis

TODO : some theory 

## Loading data for the json files 

```{r cache=TRUE}

# Load the package required to read JSON files.
library("jsonlite")
library("dplyr")
library("ggplot2")

# TODO: Set working directory to where this script is (avoid hardcoded paths)
this.dir <- "/Users/sahbi/HES/CS107/final project/cs107final/"
setwd(this.dir)

#big_json_file <- "../amazon/reviews_Electronics.json"
json_file <- "data/reviews_electronics_extract.json"
#json_file <- "data/reviews_Health_extract.json"

# Load data into dataframe. The file format is NDJSON (Newline Delimited JSON), So stream_in
# is the approriate way of doing it 

df <- stream_in(file(json_file))

#Preview the result.
glimpse(df)
```

## Exploratory Data Analysis

```{r}
library("dplyr")
library("ggplot2")

## checking how many users are there 
n_distinct(df$reviewerID)

## checking how many products are there
n_distinct(df$asin)

## looking for the number of ratings per product
ratings <- df %>% group_by(asin) %>%
       summarize(n_ratings = n() )

ratings %>%  ggplot(aes(n_ratings)) + 
  geom_histogram() + 
  scale_x_log10() +
  labs(title="Histogram for Ratings per Product") +
  labs(x="Number of Ratings", y="Count") 

## Let's see the distribution of ratings 
library(ggplot2)

df %>%  ggplot(aes(overall)) + 
geom_histogram(binwidth=1,
               col="red", 
               fill="green",
               alpha = .2) + 
labs(title="Histogram for Ratings per Product") +
labs(x="Rating Stars", y="number of reviews") 


```

> TODO : coment on this


The rating system used in Amazon 

* emotional positive (+2 or 5 stars)
* rational positive (+1 or 4 stars)
* neutral (0 or 3 stars)
* rational negative (−1 or 2 stars)
* emotional negative (−2 or 1 star)

We will collapse the 2 positive ratings (4, 5 stars) into +1 value, and the 2 negative (1, 2 stars) into Negative (-1)  and 3 becomes 0

So at the end we'll have 3 opinions : negative/positive and neutral

```{r}
overall <-seq(1,5)
rating <- c(-1,-1,0,1,1)
trans <- data.frame(cbind(overall, rating))

df2 <- df %>% inner_join(trans) %>% select(-overall)

```


Let's check again the distribution of opinions

```{r}
df2 %>%  ggplot(aes(rating)) + 
geom_histogram(binwidth=1,
               col="red", 
               fill="green",
               alpha = .2) + 
labs(title="Histogram for Ratings per Product") +
labs(x="Rating Stars", y="number of reviews") 

```




## Sentiment Analysis (Bag of Words)

One of the simpler things to do with text is to treat each text as a "bag of words.".

```{r}
library("tidytext")
sentiments
```

There are other lexicons in this dataset (see `?sentiments` for more) but we'll use the NRC dataset first. This dataset associates each word with one or more moods, such as "anger", "joy", or "sadness".

```{r}
bow <- sentiments %>%
    filter(lexicon == "nrc") %>%
    select(word, sentiment)

bow
```

Prepare the text for processing, convert it to lower case

```{r}
library("stringr")

dat <- df2 %>% mutate(reviewText = tolower(reviewText),
                        summary = tolower(summary),
                             id = row_number()) %>% 
              dplyr::select(id, reviewText, rating)


# the unnest_tokens function does not remove punctuation in this example "best.I" 
# so I'll remove them manually before


dat$reviewText = gsub( "[\\.\\,]([^ ])", "\\. \\1", dat$reviewText)

```


Now start by tokenizing the text of each review from the bag of words

```{r}
library("tidyr")
library("tidytext")

# tokenize into paragraphs then words using the unnest_tokens
# match word-sentiment pairs from bow to our reviewtext. We use inner_join function from dplyr.

words <- dat  %>%
    unnest_tokens(word, reviewText) %>%
    filter(!word %in% stop_words$word) %>%
    inner_join(bow) %>%
    count(sentiment, id) %>%
    spread(sentiment, n, fill = 0) %>%
##   mutate(positivity = (positive - negative) / (positive + negative + 1)) %>%
    
## join back to the intial data frame
   inner_join(dat) %>%
   select(-reviewText)


## if we consider only negative and positive features as  predictors

words  %>%
    ggplot(aes(positive, negative, color=factor(rating))) + 
    geom_jitter() # + scale_x_log10() + scale_y_log10()


```

Discover the top words for both positive and negative ratings. We use the `wordcloud` package to have a nice display. 


```{r warning=FALSE}
library("wordcloud")

emo_pos <- dat %>% filter(rating == 1)  %>%
            unnest_tokens(word, reviewText)  %>%
            filter(!word %in% stop_words$word) %>%
            filter(nchar(word) > 2) %>%
            count(word) 

emo_neg <- dat %>% filter(rating == -1)  %>%
            unnest_tokens(word, reviewText)  %>%
            filter(!word %in% stop_words$word) %>%
            filter(nchar(word) > 2) %>%
            count(word) 

wordcloud(words = emo_pos$word, freq = emo_pos$n, scale=c(5,0.5), max.words=100, random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))

wordcloud(words = emo_neg$word, freq = emo_neg$n, scale=c(5,0.5), max.words=100, random.order=FALSE, rot.per=0.35, use.r.layout=FALSE, colors=brewer.pal(8, "Dark2"))

```










Now that we know some basic facts about our data set, 
let's randomly split the data into training and test data. 
We set the seed `set.seed(755)` and use `sample()` function to select 
your test index to create two separate data frames
called: `train` and `test` from the original `ratings` data frame. 
 `test` contains a randomly selected 10% of the rows and training the other 90%. We will 
use these data frames to do the rest of the analyses in the problem set.


```{r}

set.seed(755)
# sample from 1 to the length of words
test_sample <- sample(1:nrow(words), nrow(words)/10) 
test <- words[test_sample,]
train <- words[-test_sample,]
# just checking the dimensions
nrow(train) / nrow(test)
#rm(words)
```


Now, we can train the naive Bayes model with the training set. We'll be using e1071 package made by David Meyer from TU Wien.
The naiveBayes function requires a 

```{r}
library("e1071")
mat_train <- as.matrix(train)

# train the model
classifier = naiveBayes(mat_train[,1:11],   ## these are being the features
                        as.factor(mat_train[,12]) )  ## this is the truth y

# test the validity
mat_test <- as.matrix(test)


predicted = predict(classifier, mat_test[,1:11])
table(as.factor(mat_test[,12]), predicted)

## calcualte accuracy

accuracy <- function(true_, predicted_) 
{
	true_ <- as.vector(true_)
	predicted_ <- as.vector(predicted_, mode=class(true_))
	res <- predicted_ == true_

	accuracy <- length(res[res == TRUE])/length(true_)
	return(accuracy)
}

accuracy(as.factor(mat_test[,12]), predicted)

## calculate RMSE

RMSE <- function(true_ratings, predicted_ratings){
    sqrt(mean((true_ratings - predicted_ratings)^2))
}


RMSE(mat_test[,12], as.numeric(predicted))


```

