<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <link href='https://fonts.googleapis.com/css?family=Chivo:900' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-dark.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <title>Sentiment Analysis of Product Review by gsahbi</title>
  </head>

  <body>
    <div id="container">
      <div class="inner">

        <header>
          <img style="float:left" src="http://i.imgur.com/wYyRKNn.png?1" alt="">
          <h1>Sentiment Analysis of Product Review</h1>
          <h2>CS107 Final Project</h2>
        </header>

        <section id="downloads" class="clearfix">
          <a href="https://github.com/gsahbi/cs107final/zipball/master" id="download-zip" class="button"><span>Download .zip</span></a>
          <a href="https://github.com/gsahbi/cs107final/tarball/master" id="download-tar-gz" class="button"><span>Download .tar.gz</span></a>
          <a href="https://github.com/gsahbi/cs107final" id="view-on-github" class="button"><span>View on GitHub</span></a>
        </section>

        <hr>

        <section id="main_content">

<h1>
<a id="project-overview" class="anchor" href="#project-overview" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>A Journey to Learning Sentiment Analysis</h1>

<hr>

<h2>
<a id="motivation-" class="anchor" href="#motivation-" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Motivation :</h2>

<p>Sentiment Analysis, is receiving a big attention these days, because of its huge spectrum of applications ranging from product review analysis, campaign feedback, competition bench-marking, customer profiles, political trends, etc...</p>

<p align="center"><img src="images/2.jpeg"></p>

<p>There is a huge flow of information going through the internet and social networks. Online discussions are only relevant to people for a couple of days. Nobody actually goes in past to tweets that are older than maybe a week, for instance. This entire humanity archive of discussion could help in many applications if we train machines to understand the sentiment of people towards a specific theme at a specific time. </p>

<h2>
<a id="background" class="anchor" href="#background" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Background</h2>

<p>“Sentiment analysis is the computational study of people's opinions, sentiments, emotions, and attitudes.” [Excerpt From: Bing Liu. Sentiment Analysis: Mining Opinions, Sentiments, and Emotions.]. This book is an excellent survey of NLP and SA research and was our refererence in this journey.</p>

<p>Given the large amount of data available on the Web, it is now possible to investigate high-level Information Retrieval tasks like user's intentions and feelings about facts or objects discussed. [Pang, B., Lee, L., 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval] </p>

<h2>
<a id="challenge-" class="anchor" href="#challenge-" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Challenge :</h2>

<p>In general, there are two main approaches when tackling SA : </p>

<ul>
<li><p><strong>Sentiment lexicons using Natural Language Processing (NLP) techniques</strong>. A Sentiment lexicon is a list of words that are associated to polarity values (positive or negative). NLP techniques offer a deep level of analysis since they take into account the context words in the sentence. </p></li>
<li><p><strong>Machine Learning classification algorithms</strong>. Because sentiment classification is a text classification problem, any existing supervised learning method can be directly applied [Bing Liu]. For example,  naive Bayes classification , logistic regression, support vector machines (SVM), etc..</p></li>
</ul>

<p>In this work we'll work on ML classification and then try to get into the NLP and experience some of the basic techniques used.</p>

<h2>
<a id="exploratory-data-analysis" class="anchor" href="#exploratory-data-analysis" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Exploratory Data Analysis</h2>

<h3>
<a id="the-data" class="anchor" href="#the-data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Data</h3>

<p>In this work we'll use a data-set that we obtained thankfully from <a href="http://cseweb.ucsd.edu/%7Ejmcauley/">Julian McAuley</a>, at the University of San Diego <a href="http://jmcauley.ucsd.edu/data/amazon/">here</a>.</p>

<p>It is worth mentioning, their excellent work in SIGIR and KDD papers (listed on the above page).</p>

<p>This data-set contains a NJSON formatted product reviews and metadata from Amazon, including 142.8 million reviews spanning May 1996 - July 2014. We have decided to use electronics reviews for this work. Because electronics are not perfect so create a lot of contrasted opinions.</p>

<h3>
<a id="basic-numbers" class="anchor" href="#basic-numbers" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Basic numbers</h3>

<p>Let's explore some facts about our data.</p>

<ul>
<li><p>The number of unique users are almost 46K for 50K reviews. So each user has done 1 unique review per product.</p></li>
<li><p>Using ASIN (Amazon Standard Identification Number) amazon's unique product identifier we find around <strong>3446</strong> products  </p></li>
</ul>

<p>Let's look at at the the number of ratings per product : </p>

            <p align="center"><img src="images/3.png"></p>

<p>It's quite skewed with some extreme best sellers (a headphone from Koss) having 3000 reviews. </p>

<h3>
<a id="understand-the-rating-system-in-amazon" class="anchor" href="#understand-the-rating-system-in-amazon" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Understand the rating system in Amazon</h3>

<p>The rating system used in Amazon is as follows : </p>

<ul>
<li>emotional positive (5 stars)</li>
<li>rational positive  (4 stars)</li>
<li>neutral            (3 stars)</li>
<li>rational negative  (2 stars)</li>
<li>emotional negative (1 star )</li>
</ul>

<p><strong>The user's star rating is an objective human interpretation of of his own review description. So, we consider that as the ground truth.</strong> </p>

<p>Let's see the distribution of these ratings in our case</p>

            <p align="center"><img src="images/4.png"></p>

<blockquote>
<p>The ratings are very skewed towards positive feedback. Which is an indication that Amazon is not selling junk at least but it's not going to help in our modeling. We have to have equal likelihood of each class of the ratings. </p>
</blockquote>

<h2>
<a id="design-decisions-on-reviews" class="anchor" href="#design-decisions-on-reviews" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Design decisions on reviews</h2>

<p>In this work we will have a binary classification. Either Positive or Negative.
We objectively chose to demarcate each rating at the 2.x level divide, such that star ratings above this level would be marked as “1” and star ratings below this level would be marked as “0” </p>

<p>We will collapse the 3, 4 and 5 stars ratings into “1” value, and the 1 and 2 stars ratings into Negative “0”</p>

<p>So at the end we'll have only 2 opinions : negative/positive</p>

            <p align="center"><img src="images/5.png"></p>

<h1>
<a id="part-i---machine-learning-classification" class="anchor" href="#part-i---machine-learning-classification" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>PART I :  Machine Learning Classification</h1>

<hr>

<h2>
<a id="bag-of-words" class="anchor" href="#bag-of-words" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Bag of Words</h2>

            <p align="center"><img src="images/6.jpeg"></p>

<p>One of the simplest things to do is to treat each text as a "bag of words". We have used the <code>tm</code> package in order to construct a Term Document Matrix but our machine couldn't handle such huge number of dimensions. So let's go simpler with <code>tidytext</code>. I fact we found that R as a framework is not as rich as Python Scikit in NLP. </p>

<h2>
<a id="feature-extraction-from-bag-of-words" class="anchor" href="#feature-extraction-from-bag-of-words" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Feature Extraction from Bag of Words</h2>

<p>Use <code>nrc</code> lexicon as a bag of words in order to remove stem words. This dataset has sentiments but we're not going to use them for the time being. </p>

<p>Let's discover the top words for both positive and negative ratings.</p>

            <p align="center"><img src="images/7.png"></p>

<h2>
<a id="tf-idf" class="anchor" href="#tf-idf" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>TF-IDF</h2>

<p>Term Frequency - Inverse Document Frequency is term count within a document weighted against the term's ubiquity within the corpus. This weight is based on the principle that terms occurring in almost every document are therefore less specific to an individual document and should be scaled down. 
So a tf-idf value represents the term's relative importance within a document.</p>

<h2>
<a id="reduce-dimensionality-single-value-decomposition" class="anchor" href="#reduce-dimensionality-single-value-decomposition" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Reduce dimensionality (Single Value Decomposition)</h2>

<p>Suppose we have m words (features) and n documents, Single value decomposition (SVD) of a tall rectangular m*n matrix is a factorization to orthogonal eigenvectors and eigenvalues. It's basically a PCA but without mean shifting. </p>

<p>SVD works better in SA because it is able to detect and extract small signals from noisy data. Noisy data here means words that are not significant for prediction.</p>

<p>In this context, it is known as latent semantic analysis (LSA).</p>

<p>The diagonal vector generated is ordered by importance of each dimension.
Let's graph a cumulative variance:  </p>

            <p align="center"><img src="images/8.png"></p>

<blockquote>
<p>Design decision</p>

<p>We have to decide how many dimensions to keep. Looking at the graph we can keep 10% dimensions which represent almost 99% of the features.
This is curious and a puzzling choice to make. But let's aim for fast processing and later on see if adding more dimensions improves our prediction.</p>
</blockquote>

<h2>
<a id="supervised-training" class="anchor" href="#supervised-training" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Supervised training</h2>

<p>As with any ML problem, we randomly split the data into training and test data. We chose to have 80% for training and 20% for testing.
Below is the list of algorithms we used :</p>

<ul>
<li><p><strong>Naive Bayes</strong>, making the assumption that the probability of each word occurring for a given class is independent.</p></li>
<li><p><strong>Support Vector Machine</strong> , SVMs were developed by Cortes &amp; Vapnik (1995) [1] for binary classification. Their approach may be roughly sketched as follows:</p></li>
</ul>

<blockquote>
<p>Class separation: basically, we are looking for the optimal separating hyperplane between the two classes by maximizing the margin between the classes’ closest points —the points lying on the boundaries are called support vectors, and the middle of the margin is our optimal separating hyperplane;</p>
</blockquote>

<ul>
<li><p><strong>Logistic regression</strong></p></li>
<li><p><strong>Random Forest</strong> where prediction function works similarly to decision trees</p></li>
</ul>

<h2>
<a id="conclusion" class="anchor" href="#conclusion" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Conclusion</h2>

<p>In this section we used a supervised machine learning approach and tried several classification models. From the table below, the algorithmic approach using randomForest gives the best accuracy. we are 25% better than flipping a coin with only a classified bag of words.</p>

<table>
<thead>
<tr>
<th align="left">Method</th>
<th align="right">Accuracy</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">Naive Bayes</td>
<td align="right">0.6670996</td>
</tr>
<tr>
<td align="left">Support Vector Machine</td>
<td align="right">0.7367965</td>
</tr>
<tr>
<td align="left">Linear Discriminant Analysis</td>
<td align="right">0.7441558</td>
</tr>
<tr>
<td align="left">Random Forest</td>
<td align="right">0.7584416</td>
</tr>
</tbody>
</table>
<br>
<br>
<br>
<h1>
<a id="part-ii--sentiment-lexicons-using-natural-language-processing" class="anchor" href="#part-ii--sentiment-lexicons-using-natural-language-processing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>PART II : Sentiment lexicons using Natural Language Processing</h1>

<hr>

<h2>
<a id="background-1" class="anchor" href="#background-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Background</h2>

<p>In the machine learning classification models based on words we've seen that many false positives originate from negations. such as "not good", "don't recommend", etc..</p>

<p>In our work, we'll use the openNLP package. Which is an API for the apache OpenNLP framework written in Java. openNLP requires a pre-made models. These are conveniently available to R by installing the respective <code>openNLPmodels.language</code> package from the repository at <a href="http://datacube.wu.ac.at">http://datacube.wu.ac.at</a></p>

<h2>
<a id="process" class="anchor" href="#process" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Process</h2>

<p>The following flowchart depicts the proposed process for categorization using NLP</p>

            <p align="center"><img src="images/9.png"></p>

<h2>
<a id="data-review" class="anchor" href="#data-review" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data review</h2>

<p>As with any text, we must do a series of transformations :</p>

<ul>
<li>lowercase</li>
<li>remove punctuation</li>
<li>strip white space</li>
</ul>

<h2>
<a id="no-brainer-classification-using-lexicons" class="anchor" href="#no-brainer-classification-using-lexicons" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>No-brainer classification using lexicons</h2>

<p>Let's do a basic sentiment analysis from a categorized bag of words, we'll use the Bing lexicon provided by <code>tidytext</code> package :</p>

<h3>
<a id="classification" class="anchor" href="#classification" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Classification</h3>

<p>The score is obtained from simply </p>



            <p align="center"><img src="images/10.png"></p>

<p>In the following figure we notice that the red and blue spaces (of +/- classes) are perfectly overlapping which explains why applying a scoring gets an accuracy as low as <strong>67%</strong> </p>

            <p align="center"><img src="images/11.png"></p>

<h3>
<a id="evaluation-of-the-results" class="anchor" href="#evaluation-of-the-results" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Evaluation of the results</h3>

<p>To find out what we can enhance, let's visualize the word distribution :</p>

            <p align="center"><img src="images/12.png"></p>

<p>Many positive words like "recommend" are found in negative ratings as well. This is due to the negations and polarity inverter adjectives. For example : Not bad, would account for negative just because of the word "bad" is there.</p>

<p>In the next section, we'll try to use NLP to enhance our classification.</p>

<h2>
<a id="a-more-sophisticated-classification" class="anchor" href="#a-more-sophisticated-classification" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>A more sophisticated Classification</h2>

<p>In the following, we'll experiment with a 1-star rated wireless charger from Amazon. This particular review would count for a positive prediction using models that do not take into account the semantics:</p>

<blockquote>
<p><em>"Bought it in Black Friday sales for good price.I don't like it.not wanna recommend to anybody"</em></p>
</blockquote>

<h3>
<a id="sentence-chunking-and-part-of-speech-tagging" class="anchor" href="#sentence-chunking-and-part-of-speech-tagging" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Sentence chunking and Part-of-Speech tagging</h3>

<p>The first step is to extract 2 types of phrases :</p>

<ul>
<li>Verbal phrases  that may imply opinions. Example : "I didn't like the design"</li>
<li>Noun phrases that may describe the product. Example : The design was not good""</li>
</ul>

<p>The part-of-speech tags meaning is found in the <a href="https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html">Penn Treebank Project</a></p>

<p><strong>Treebank of Sentence and POS tagging</strong></p>

            <p align="center"><img src="images/14.png"></p>

<h3>
<a id="feature-identification" class="anchor" href="#feature-identification" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Feature identification</h3>

<p>For the purpose of this work we won't go for a full analysis using penn treebanks. Rather we'll simply identify the following words inside a verbal and noun chunks (phrase):</p>

<ul>
<li>[VB] : verb w/ positive/negative sentiment : like, hate , etc</li>
<li>[JJ] : adjective w/ positive/negative sentiment : bad, junk</li>
<li>[RB] : adverb polarity inverter such as : n't, or incr/decrementers such as : too, very, more etc.</li>
</ul>

<h2>
<a id="scoring-algorithm" class="anchor" href="#scoring-algorithm" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Scoring algorithm</h2>

<p>The idea is to find RB multiplier and look for a VB or JJ surrounding it :
For example : n't like : will compute 1 x -1</p>

<p>We should be able to calculate our sentiment score as the diff between negative and positive counts normalized by the count just like we did before.</p>

<h2>
<a id="interpretation" class="anchor" href="#interpretation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Interpretation</h2>

<p>Just as before we can plot these 2 positive and negative dimensions :</p>

            <p align="center"><img src="images/15.png"></p>

<p>Notice how the discriminance of the 2 clusters is higher.</p>

<h1>
<a id="conclusion-1" class="anchor" href="#conclusion-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Conclusion</h1>
            <p align="center"><img src="images/13.png"></p>

<p>Some of the basic techniques of NLP can enhance the accuracy of sentiment scoring. We went from 68% to 74% with a simple identification of negation-of-adjective (NOA) and negation-of-verb (NOV). And using a randomForest algorithm we achieved nearly 78% accuracy.</p>

<p>But, some language forms require a deep analysis. For example :</p>

<p>Sarcasm : "Great sound when working"
Deep inverter : "Not such a good value after all", polarity inverter adverb is 3-words faraway from its adjective.</p>
        </section>

        <footer>
          Sentiment Analysis of Product Review is maintained by <a href="https://github.com/gsahbi">gsahbi</a><br>
          This page was generated by <a href="https://pages.github.com">GitHub Pages</a>. Tactile theme by <a href="https://twitter.com/jasonlong">Jason Long</a>.
        </footer>

        
      </div>
    </div>
  </body>
</html>
